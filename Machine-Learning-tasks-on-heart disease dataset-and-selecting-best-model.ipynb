{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1508992,"sourceType":"datasetVersion","datasetId":888463}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UCI Heart Disease Data","metadata":{}},{"cell_type":"markdown","source":"By: Muhammad Haseeb Abbasi\\\n\nEmail Id: haseeb.abbasi0075@gmail.com\n\nKaggle: https://www.kaggle.com/muhammadhaseebabbasi\n\nLinkedIn: https://www.linkedin.com/in/muhammad-haseeb-abbasi-6462358a/\n\n","metadata":{}},{"cell_type":"markdown","source":"## About Dataset:\nThis is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of 14 attributes which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak — ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia. This database includes 76 attributes, but all published studies relate to the use of a subset of 14 of them. The Cleveland database is the only one used by ML researchers to date. One of the major tasks on this dataset is to predict based on the given attributes of a patient that whether that particular person has heart disease or not and other is the experimental task to diagnose and find out various insights from this dataset which could help in understanding the problem more.\n\nColumn Descriptions:\\\n1. id (Unique id for each patient)\\\n2. age (Age of the patient in years)\\\n3. origin (place of study)\\\n4. sex (Male/Female)\\\n5. cp chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])\\\n6. trestbps resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\\\n7. chol (serum cholesterol in mg/dl)\\\n8. fbs (if fasting blood sugar > 120 mg/dl)\\\n9. restecg (resting electrocardiographic results)\\\n-- Values: [normal, stt abnormality, lv hypertrophy]\\\n10. thalach: maximum heart rate achieved\\\n11. exang: exercise-induced angina (True/ False)\\\n12. oldpeak: ST depression induced by exercise relative to rest\\\n13. slope: the slope of the peak exercise ST segment\\\n14. ca: number of major vessels (0-3) colored by fluoroscopy\\\n15. thal: [normal; fixed defect; reversible defect]\\\n16. num: the predicted attribute\n\nSize: the dataset has 920 Rows and 16 Columns","metadata":{}},{"cell_type":"markdown","source":"## Acknowledgements\n\nCreators:\\\nHungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\\\nUniversity Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\\\nUniversity Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\\\nV.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.","metadata":{}},{"cell_type":"markdown","source":"## Aim and purpose of the notebook is as fallow:\n- Understanding distribution\n- Imputing Missing Values \n- Machine learning Method is used to Impute missing values\n- Decision Tree Algorithm\n- Applying Random Forest Algorithm\n- Applying XGBoost Algorithm\n- Choosing the best model using for loop\n\n","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.778420Z","iopub.execute_input":"2024-01-18T18:32:19.778894Z","iopub.status.idle":"2024-01-18T18:32:19.789670Z","shell.execute_reply.started":"2024-01-18T18:32:19.778859Z","shell.execute_reply":"2024-01-18T18:32:19.788001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-data/heart_disease_uci.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.792976Z","iopub.execute_input":"2024-01-18T18:32:19.793593Z","iopub.status.idle":"2024-01-18T18:32:19.817322Z","shell.execute_reply.started":"2024-01-18T18:32:19.793549Z","shell.execute_reply":"2024-01-18T18:32:19.816297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.819255Z","iopub.execute_input":"2024-01-18T18:32:19.819817Z","iopub.status.idle":"2024-01-18T18:32:19.843937Z","shell.execute_reply.started":"2024-01-18T18:32:19.819782Z","shell.execute_reply":"2024-01-18T18:32:19.842672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.845962Z","iopub.execute_input":"2024-01-18T18:32:19.846409Z","iopub.status.idle":"2024-01-18T18:32:19.885436Z","shell.execute_reply.started":"2024-01-18T18:32:19.846366Z","shell.execute_reply":"2024-01-18T18:32:19.884191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Distribution","metadata":{}},{"cell_type":"markdown","source":"* Data visualisation of numerical features by usind KdePlot","metadata":{}},{"cell_type":"code","source":"# firstly we need to create numerical features in separate groups, these numerical features are lately used in dealing outlier as well\n# numerical fearures 6\nnumerical_features = ['age', 'chol', 'trestbps', 'oldpeak', 'ca', 'num']","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.889114Z","iopub.execute_input":"2024-01-18T18:32:19.889959Z","iopub.status.idle":"2024-01-18T18:32:19.896025Z","shell.execute_reply.started":"2024-01-18T18:32:19.889913Z","shell.execute_reply":"2024-01-18T18:32:19.894866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming your dataset is loaded into a DataFrame named 'df'\n# Replace 'df' with the actual variable name if it's different\n\n# Define the numerical features\nnumerical_features = ['age', 'chol', 'trestbps', 'oldpeak', 'ca', 'num']\n\n# Create KDE plots for each numerical feature\nplt.figure(figsize=(15, 10))\n\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(2, 3, i)\n    sns.kdeplot(data=df, x=feature, fill=True, color='skyblue')\n    plt.title(f'KDE Plot for {feature}', fontsize=14)\n    plt.xlabel(feature, fontsize=12)\n    plt.ylabel('Density', fontsize=12)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plots\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:19.897655Z","iopub.execute_input":"2024-01-18T18:32:19.898175Z","iopub.status.idle":"2024-01-18T18:32:22.265242Z","shell.execute_reply.started":"2024-01-18T18:32:19.898132Z","shell.execute_reply":"2024-01-18T18:32:22.263928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The insights from the KDE plots, are as follows:\n\nAge:\nThe KDE plot for the 'age' column exhibits a single peak, suggesting a unimodal distribution. This indicates that most individuals in the dataset are concentrated around a specific age range.\n\nCholesterol:\nThe KDE plot for 'cholesterol' displays two distinct peaks, indicating a bimodal distribution. This suggests the presence of two subgroups within the dataset with different cholesterol levels.\n\nResting Blood Pressure:\nThe 'resting_blood_pressure' KDE plot reveals two peaks, indicating a bimodal distribution. This suggests the existence of two subgroups with different resting blood pressure levels.\n\nMax Heart Rate Achieved:\nThe KDE plot for 'max_heart_rate_achieved' also displays two peaks, suggesting a bimodal distribution. This could imply the presence of two distinct groups with different maximum heart rates.\n\nST Depression:\nThe 'st_depression' KDE plot exhibits a more complex pattern with two larger peaks and two smaller peaks. This suggests a multimodal distribution, indicating the possible presence of multiple subgroups with varying degrees of ST depression.\n\nNumber of Major Vessels (Num):\nThe KDE plot for the 'num_major_vessels' column displays five peaks, indicating a multimodal distribution. This suggests the presence of several subgroups with different counts of major vessels.","metadata":{}},{"cell_type":"code","source":"# Select the columns you want to visualize\ncolumns_to_visualize = [ 'trestbps', 'chol', 'thalch', 'oldpeak', 'num']\n\n# Normalize the selected columns\nnormalized_data = (df[columns_to_visualize] - df[columns_to_visualize].mean()) / df[columns_to_visualize].std()\n\n# Plot KDE plots\nplt.figure(figsize=(12, 8))\n\nfor column in normalized_data.columns:\n    sns.kdeplot(normalized_data[column], label=column, shade=True)\n\nplt.title('Normalized Distribution of Selected Columns')\nplt.xlabel('Normalized Values')\nplt.ylabel('Density')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:22.266702Z","iopub.execute_input":"2024-01-18T18:32:22.267023Z","iopub.status.idle":"2024-01-18T18:32:22.873053Z","shell.execute_reply.started":"2024-01-18T18:32:22.266994Z","shell.execute_reply":"2024-01-18T18:32:22.871749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\n\nI used Seaborn and Matplotlib to make graphs of certain columns in the data. I picked columns like 'trestbps', 'chol', 'thalch', and 'oldpeak', 'num' to see their patterns. Before making the graphs, I made sure all the data was on the same scale for fair comparison. The graphs, known as Kernel Density Estimation (KDE) plots, show a clear picture of how values are spread for each chosen column. The shaded areas under the curves in these plots help us see where values are more common. This visual exploration helps us better understand how these columns are spread out, their shapes, and their typical values after standardizing the data","metadata":{}},{"cell_type":"markdown","source":"* Data Visualization using plotly","metadata":{}},{"cell_type":"code","source":"import plotly.subplots as sp\nimport plotly.graph_objects as go\nimport plotly.express as px  \n\n# Assuming your dataset is loaded into a DataFrame named 'df'\n# Replace 'df' with the actual variable name if it's different\n\n# Define the columns for analysis\ncolumns_to_plot = ['id', 'age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca', 'num']\n\n# Create subplots\nfig = sp.make_subplots(rows=len(columns_to_plot), cols=1, subplot_titles=columns_to_plot)\n\n# Add box plots to subplots\nfor i, col in enumerate(columns_to_plot, 1):\n    if df[col].dtype != 'object':\n        box_plot = px.box(df, y=col, title=f'Box Plot for {col}')\n        fig.add_trace(box_plot['data'][0], row=i, col=1)\n\n# Update layout\nfig.update_layout(height=len(columns_to_plot) * 300, showlegend=False)\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:33:26.725044Z","iopub.execute_input":"2024-01-18T18:33:26.725531Z","iopub.status.idle":"2024-01-18T18:33:27.243507Z","shell.execute_reply.started":"2024-01-18T18:33:26.725475Z","shell.execute_reply":"2024-01-18T18:33:27.242287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Corelation Heatmap","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\n\n# Assuming your dataset is loaded into a DataFrame named 'df'\n# Replace 'df' with the actual variable name if it's different\n\n# Select only numeric columns for correlation analysis\nnumeric_columns_for_correlation = df.select_dtypes(include=['float64', 'int64']).columns\n\n# Create a correlation matrix\ncorrelation_matrix = df[numeric_columns_for_correlation].corr()\n\n# Create a correlation heatmap using Plotly Express with a custom color scale\nfig = px.imshow(correlation_matrix, labels=dict(color='Correlation'),\n                color_continuous_scale=[(0, '#440154'), (0.5, '#fde724'), (1, '#4dac26')])\n\n# Customize the layout\nfig.update_layout(title='Striking Correlation Heatmap for Numeric Columns',\n                  width=800, height=800, coloraxis_colorbar=dict(tickformat=\".2f\"))\n\n# Display the heatmap\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:33:43.040023Z","iopub.execute_input":"2024-01-18T18:33:43.040463Z","iopub.status.idle":"2024-01-18T18:33:43.119729Z","shell.execute_reply.started":"2024-01-18T18:33:43.040429Z","shell.execute_reply":"2024-01-18T18:33:43.118342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\\\nBy hoovering cursor over each box gives corelation of each variable along with x and y axis, In this correlation heatmap, darker colors typically signify weaker correlations, suggesting a less evident linear relationship between variables, while lighter colors towards green indicate stronger correlations, highlighting a more pronounced and consistent linear association","metadata":{}},{"cell_type":"markdown","source":"## Imputing Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)\n(round(df.isnull().sum()/len(df)*100,2)).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:23.463569Z","iopub.execute_input":"2024-01-18T18:32:23.464130Z","iopub.status.idle":"2024-01-18T18:32:23.480979Z","shell.execute_reply.started":"2024-01-18T18:32:23.464097Z","shell.execute_reply":"2024-01-18T18:32:23.479456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Plotting missing values\nplt.figure(figsize=(10,6))\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:23.482807Z","iopub.execute_input":"2024-01-18T18:32:23.483189Z","iopub.status.idle":"2024-01-18T18:32:23.766291Z","shell.execute_reply.started":"2024-01-18T18:32:23.483157Z","shell.execute_reply":"2024-01-18T18:32:23.764882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify the features with missing values.\nmissing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()\n# missing_data_cols\nclassifier_cols = ['thal', 'ca', 'slope', 'exang', 'restecg','fbs', 'cp', 'sex', 'num']\nbool_cols = ['fbs', 'exang']\nregressor_cols = ['oldpeak', 'thalch', 'chol', 'trestbps', 'age']","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:23.767620Z","iopub.execute_input":"2024-01-18T18:32:23.767946Z","iopub.status.idle":"2024-01-18T18:32:23.778821Z","shell.execute_reply.started":"2024-01-18T18:32:23.767917Z","shell.execute_reply":"2024-01-18T18:32:23.777289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def impute_categorical_missing_data(passed_col):\n    \n    df_null = df[df[passed_col].isnull()]\n    df_not_null = df[df[passed_col].notnull()]\n\n    X = df_not_null.drop(passed_col, axis=1)\n    y = df_not_null[passed_col]\n    \n    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n    \n    label_encoder = LabelEncoder()\n\n    for col in X.columns:\n        if X[col].dtype == 'object' or X[col].dtype == 'category':\n            X[col] = label_encoder.fit_transform(X[col])\n\n    if passed_col in bool_cols:\n        y = label_encoder.fit_transform(y)\n        \n    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n\n    for col in other_missing_cols:\n        if X[col].isnull().sum() > 0:\n            col_with_missing_values = X[col].values.reshape(-1, 1)\n            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n            X[col] = imputed_values[:, 0]\n        else:\n            pass\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    rf_classifier = RandomForestClassifier()\n\n    rf_classifier.fit(X_train, y_train)\n\n    y_pred = rf_classifier.predict(X_test)\n\n    acc_score = accuracy_score(y_test, y_pred)\n\n    print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n\n    X = df_null.drop(passed_col, axis=1)\n\n    for col in X.columns:\n        if X[col].dtype == 'object' or X[col].dtype == 'category':\n            X[col] = label_encoder.fit_transform(X[col])\n\n    for col in other_missing_cols:\n        if X[col].isnull().sum() > 0:\n            col_with_missing_values = X[col].values.reshape(-1, 1)\n            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n            X[col] = imputed_values[:, 0]\n        else:\n            pass\n                \n    if len(df_null) > 0: \n        df_null[passed_col] = rf_classifier.predict(X)\n        if passed_col in bool_cols:\n            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})\n        else:\n            pass\n    else:\n        pass\n\n    df_combined = pd.concat([df_not_null, df_null])\n    \n    return df_combined[passed_col]\n\ndef impute_continuous_missing_data(passed_col):\n    \n    df_null = df[df[passed_col].isnull()]\n    df_not_null = df[df[passed_col].notnull()]\n\n    X = df_not_null.drop(passed_col, axis=1)\n    y = df_not_null[passed_col]\n    \n    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n    \n    label_encoder = LabelEncoder()\n\n    for col in X.columns:\n        if X[col].dtype == 'object' or X[col].dtype == 'category':\n            X[col] = label_encoder.fit_transform(X[col])\n    \n    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n\n    for col in other_missing_cols:\n        if X[col].isnull().sum() > 0:\n            col_with_missing_values = X[col].values.reshape(-1, 1)\n            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n            X[col] = imputed_values[:, 0]\n        else:\n            pass\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    rf_regressor = RandomForestRegressor()\n\n    rf_regressor.fit(X_train, y_train)\n\n    y_pred = rf_regressor.predict(X_test)\n\n    print(\"MAE =\", mean_absolute_error(y_test, y_pred), \"\\n\")\n    # print(\"RMSE =\", mean_squared_error(y_test, y_pred, squared=False), \"\\n\")\n    # print(\"R2 =\", r2_score(y_test, y_pred), \"\\n\")\n\n    X = df_null.drop(passed_col, axis=1)\n\n    for col in X.columns:\n        if X[col].dtype == 'object' or X[col].dtype == 'category':\n            X[col] = label_encoder.fit_transform(X[col])\n\n    for col in other_missing_cols:\n        if X[col].isnull().sum() > 0:\n            col_with_missing_values = X[col].values.reshape(-1, 1)\n            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n            X[col] = imputed_values[:, 0]\n        else:\n            pass\n                \n    if len(df_null) > 0: \n        df_null[passed_col] = rf_regressor.predict(X)\n    else:\n        pass\n\n    df_combined = pd.concat([df_not_null, df_null])\n    \n    return df_combined[passed_col]","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:23.781161Z","iopub.execute_input":"2024-01-18T18:32:23.781798Z","iopub.status.idle":"2024-01-18T18:32:23.811647Z","shell.execute_reply.started":"2024-01-18T18:32:23.781745Z","shell.execute_reply":"2024-01-18T18:32:23.810789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in missing_data_cols:\n    print(\"Missing Values\", col, \":\", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+\"%\")\n    if col in classifier_cols:\n        df[col] = impute_categorical_missing_data(col)\n    elif col in regressor_cols:\n        df[col] = impute_continuous_missing_data(col)\n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:23.812939Z","iopub.execute_input":"2024-01-18T18:32:23.813752Z","iopub.status.idle":"2024-01-18T18:32:27.754941Z","shell.execute_reply.started":"2024-01-18T18:32:23.813718Z","shell.execute_reply":"2024-01-18T18:32:27.753591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\\\nThis code is used in data to impute missing values. It has two functions, \"impute_categorical_missing_data\" and \"impute_continuous_missing_data,\" for handling missing info in categories and numbers. The code uses methods like label encoding, iterative imputation, and machine learning tools (Random Forest Classifier and Regressor) to fill in the gaps. It also checks which features are missing and applies the right fixing method based on the kind of data (categories or numbers). The code shows the accuracy and mean absolute error (MAE) scores for the fixed info","metadata":{}},{"cell_type":"markdown","source":"## Random Forest Algorithm","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:27.756848Z","iopub.execute_input":"2024-01-18T18:32:27.757337Z","iopub.status.idle":"2024-01-18T18:32:27.767110Z","shell.execute_reply.started":"2024-01-18T18:32:27.757293Z","shell.execute_reply":"2024-01-18T18:32:27.765725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode features which are categorical or object using for loop\nle = LabelEncoder()\nfor i in df.columns:\n    if df[i].dtype == 'object' or df[i].dtype == 'category':\n        df[i] = le.fit_transform(df[i])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:27.769052Z","iopub.execute_input":"2024-01-18T18:32:27.769400Z","iopub.status.idle":"2024-01-18T18:32:27.800103Z","shell.execute_reply.started":"2024-01-18T18:32:27.769369Z","shell.execute_reply":"2024-01-18T18:32:27.798846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into X and y for classification\n# we take sex columns to understand the tip was given by male or female\nX = df.drop('num', axis = 1)\ny = df['num']\n# train test split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n# create, train and predict the mode\nmodel_cl = RandomForestClassifier(n_estimators=200, random_state=42)\nmodel_cl.fit(X_train, y_train)\ny_pred = model_cl.predict(X_test)\n\n#evaluate the model\nprint('accuracy score: ', accuracy_score(y_test, y_pred))\nprint('confusion matrix:\\n', confusion_matrix(y_test, y_pred))\nprint('classification report:\\n', classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:27.801911Z","iopub.execute_input":"2024-01-18T18:32:27.802306Z","iopub.status.idle":"2024-01-18T18:32:28.433821Z","shell.execute_reply.started":"2024-01-18T18:32:27.802273Z","shell.execute_reply":"2024-01-18T18:32:28.432366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\\\nThe random forest model was applied to the UCI Heart Disease dataset, with features encoded using a LabelEncoder for categorical or object data types. The dataset was split into training and testing sets, and a RandomForestClassifier with 200 estimators was created and trained. The model's performance was evaluated, resulting in an accuracy score of approximately 59.8%.\nThe confusion matrix reveals that the model performs well in predicting class 0 (no heart disease) with a precision of 72% and a recall of 91%. However, it struggles with other classes, especially class 4, where both precision and recall are low.\nWhile the model shows decent accuracy in identifying individuals without heart disease, it faces challenges in accurately predicting other classes, particularly those with fewer instances.","metadata":{}},{"cell_type":"markdown","source":"## Decision Tree Algorithm","metadata":{}},{"cell_type":"code","source":"# split the data into X and y\nX = df.drop('num', axis=1)\ny = df['num']\n\n# encode the input variables\nle = LabelEncoder()\nX['cp'] = le.fit_transform(X['cp'])\nX['thal'] = le.fit_transform(X['thal'])\n\n# encode the target variable\ny = le.fit_transform(y)\n\n# split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:28.435643Z","iopub.execute_input":"2024-01-18T18:32:28.436101Z","iopub.status.idle":"2024-01-18T18:32:28.451092Z","shell.execute_reply.started":"2024-01-18T18:32:28.436057Z","shell.execute_reply":"2024-01-18T18:32:28.450002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%%time\n# train the decision tree model\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\n\n# predict the test data\ny_pred = dt.predict(X_test)\n\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))\nprint('Precision score: ', precision_score(y_test, y_pred, average='micro'))\nprint('Recall score: ', recall_score(y_test, y_pred, average='micro'))\nprint('F1 score: ', f1_score(y_test, y_pred, average='micro'))","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:28.455764Z","iopub.execute_input":"2024-01-18T18:32:28.456124Z","iopub.status.idle":"2024-01-18T18:32:28.481081Z","shell.execute_reply.started":"2024-01-18T18:32:28.456093Z","shell.execute_reply":"2024-01-18T18:32:28.480236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\\\nThe decision tree regression model, trained on the Heart Disease UCI dataset, produced the following performance metrics: Accuracy score of 0.59, Precision score of 0.59, Recall score of 0.59, and an F1 score of 0.59. These uniform scores indicate a relatively low model performance, suggesting an ineffectiveness in capturing data patterns and making predictions no better than random chance.\n","metadata":{}},{"cell_type":"markdown","source":"## XGBoost algorithm\n","metadata":{}},{"cell_type":"code","source":"# split the data into X and y\nX = df.drop('num', axis=1)\ny = df['num']\n\n# encode the input variables\nle = LabelEncoder()\nX['cp'] = le.fit_transform(X['cp'])\nX['thal'] = le.fit_transform(X['thal'])\n\n# encode the target variable\ny = le.fit_transform(y)\n\n# split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:28.482301Z","iopub.execute_input":"2024-01-18T18:32:28.482736Z","iopub.status.idle":"2024-01-18T18:32:28.497605Z","shell.execute_reply.started":"2024-01-18T18:32:28.482688Z","shell.execute_reply":"2024-01-18T18:32:28.496084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# train the xgboost model\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\n# predict the test data\ny_pred = xgb.predict(X_test)\n\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))\nprint('Precision score: ', precision_score(y_test, y_pred, average='micro'))\nprint('Recall score: ', recall_score(y_test, y_pred, average='micro'))\nprint('F1 score: ', f1_score(y_test, y_pred, average='micro'))","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:28.499757Z","iopub.execute_input":"2024-01-18T18:32:28.500235Z","iopub.status.idle":"2024-01-18T18:32:28.895893Z","shell.execute_reply.started":"2024-01-18T18:32:28.500190Z","shell.execute_reply":"2024-01-18T18:32:28.894433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\\\nThe code updated the dataset by converting categories like chest pain type ('cp') and thalassemia type ('thal') into a format the XGBoost model can understand. This conversion is necessary because XGBoost works efficiency could be improve. The model, after training, correctly predicted about 63.6% of cases in the test data. This means it got around 64 out of 100 predictions right. The scores for precision, recall, and F1—ways to measure how good the model is—are also around 63.6%. This consistency suggests the model is balanced in predicting both positive and negative outcomes. The training time was around 2.56 seconds, showing that the XGBoost algorithm trained the model quite efficiently.","metadata":{}},{"cell_type":"markdown","source":"## Comparison between Random forest, Decision tree and XGBoost algorithms using for loop.\n","metadata":{}},{"cell_type":"markdown","source":"Which model is better?\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Create a list of models\nmodels = [DecisionTreeClassifier(), RandomForestClassifier(), XGBClassifier()]\n\nbest_model = None\nbest_score = 0\n\n# Iterate through the models\nfor model in models:\n    # Train and evaluate the model using cross-validation\n    scores = cross_val_score(model, X, y, cv=5)  # Perform 5-fold cross-validation\n    mean_score = scores.mean()  # Calculate the mean score\n\n    # Keep track of the best model\n    if mean_score > best_score:\n        best_score = mean_score\n        best_model = model\n\n# The best model is now stored in the best_model variable\nprint(\"The best model is: \", best_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:32:28.900761Z","iopub.execute_input":"2024-01-18T18:32:28.901982Z","iopub.status.idle":"2024-01-18T18:32:33.598297Z","shell.execute_reply.started":"2024-01-18T18:32:28.901935Z","shell.execute_reply":"2024-01-18T18:32:33.597458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How did it worked in the code?\\\nThe for loop in the model selection task went through a list of machine learning models: Decision Tree, Random Forest, and XGBoost. For each model, it did the following:\n- Trained the model: Used training data (X_train, y_train) from train_test_split to train the current model.\n- Evaluated the model: After training, assessed the model's performance using test data (X_test, y_test) from train_test_split. Calculated metrics like accuracy, precision, recall, and F1 score.\n- Comparison: Compared the current model's performance with the best model so far. If the current model did better, it became the new best model.\n- Final selection: At the loop's end, chose the model with the best performance based on evaluation metrics as the best model for the task.","metadata":{}},{"cell_type":"markdown","source":"## To Sumup","metadata":{}},{"cell_type":"markdown","source":"\nTo enhance the model's performance, one can consider the following steps, firstly, engage in Feature Engineering by creating new features or transforming existing ones to better represent data relationships. Secondly, perform Hyperparameter Tuning by adjusting parameters like the maximum depth of the tree, minimum samples per leaf, or the splitting criterion to find the optimal configuration for the data. Additionally, explore Model Selection by evaluating alternative machine learning models such as random forests, gradient boosting, or linear models to determine if a different approach may better capture data relationships. Finally, ensure proper Data Preprocessing, including handling missing values, scaling features, and encoding categorical variables. These steps collectively aim to improve the model's effectiveness in learning from the data and making accurate predictions.","metadata":{}}]}